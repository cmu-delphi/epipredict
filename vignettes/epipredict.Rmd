---
title: "Get started with `{epipredict}`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Get started with `{epipredict}`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
source(here::here("vignettes/_common.R"))
```

```{r setup, message=FALSE, include = FALSE}
library(dplyr)
library(parsnip)
library(workflows)
library(recipes)
library(epidatasets)
library(epipredict)
library(epiprocess)
library(ggplot2)
library(purrr)
forecast_date <- as.Date("2021-08-01")
used_locations <- c("ca", "ma", "ny", "tx")
library(epidatr)
```

At a high level, the goal of `{epipredict}` is to make it easy to run simple machine
learning and statistical forecasters for epidemiological data.
To do this, we have extended the [tidymodels](https://www.tidymodels.org/)
framework to handle the case of panel time-series data.

Our hope is that it is easy for users with epidemiological training and some statistical knowledge to
fit baseline models, while also allowing those with more nuanced statistical
understanding to create complex custom models using the same framework.
Towards that end, `{epipredict}` provides two main classes of tools:

## Canned forecasters

A set of basic, easy-to-use "canned" forecasters that work out of the box.
We currently provide the following basic forecasters:
  
  * _Flatline forecaster_: predicts as the median the most recently seen value 
    with increasingly wide quantiles.
  * _Autoregressive forecaster_: fits a model (e.g. linear regression) on
    lagged data to predict quantiles for continuous values.
  * _Autoregressive classifier_: fits a model (e.g. logistic regression) on
    lagged data to predict a binned version of the growth rate.
  * _CDC FluSight flatline forecaster_: a variant of the flatline forecaster that is
    used as a baseline in the CDC's [FluSight forecasting competition](https://www.cdc.gov/flu-forecasting/about/index.html).

## Forecasting framework

A framework for creating custom forecasters out of modular components, from
which the canned forecasters were created.  There are three types of
components:
 
  * _Preprocessor_: transform the data before model training, such as converting
    counts to rates, creating smoothed columns, or [any `{recipes}`
    `step`](https://recipes.tidymodels.org/reference/index.html)
  * _Trainer_: train a model on data, resulting in a fitted model object.
    Examples include linear regression, quantile regression, or [any `{parsnip}`
    engine](https://parsnip.tidymodels.org/reference/index.html).
  * _Postprocessor_: unique to `{epipredict}`; used to transform the
    predictions after the model has been fit, such as
    - generating quantiles from purely point-prediction models,
    - reverting operations done in the `step`s, such as converting from
    rates back to counts
    - generally adapting the format of the prediction to its eventual use.

The rest of the "getting started" vignette will focus on using and modifying the canned forecasters.
Check out the ["guts" vignette](preprocessing-and-models) for examples of using the forecaster
framework to make more complex, custom forecasters.

If you are interested in time series in a non-panel data context, you may also
want to look at `{timetk}` and `{modeltime}` for some related techniques.

For a more in-depth treatment with some practical applications, see also the
[Forecasting Book](https://cmu-delphi.github.io/delphi-tooling-book/).

# Panel forecasting basics
## Example data

The forecasting methods in this package are designed to work with panel time
series data in `epi_df` format as made available in the `{epiprocess}`
package.
An `epi_df` is a collection of one or more time-series indexed by one or more
categorical variables.
The [`{epidatasets}`](https://cmu-delphi.github.io/epidatasets/) package makes several
pre-compiled example datasets available.
Let's look at an example `epi_df`:

```{r data_ex}
covid_case_death_rates
```

This dataset uses a single key, `geo_value`, and two separate
time series, `case_rate` and `death_rate`.
The keys are represented in "long" format, with separate columns for the key and
the value, while separate time series are represented in "wide" format with each
time series stored in a separate column.

`{epiprocess}` is designed to handle data that always has a geographic key, and
potentially other key values, such as age, ethnicity, or other demographic
information.
For example, `grad_employ_subset` from `{epidatasets}` also has both `age_group`
and `edu_qual` as additional keys:

```{r extra_keys}
grad_employ_subset
```

See `{epiprocess}` for [more details on the `epi_df` format](https://cmu-delphi.github.io/epiprocess/articles/epi_df.html).

Panel time series are ubiquitous in epidemiology, but are also common in
economics, psychology, sociology, and many other areas.
While this package was designed with epidemiology in mind, many of the
techniques are more broadly applicable.

## Customizing `arx_forecaster()`
Let's expand on the basic example presented on the [landing
page](../index.html#motivating-example), starting with adjusting some parameters in
`arx_forecaster()`.

The `trainer` argument allows us to set the fitting engine. We can use either one of the
included engines, such as `quantile_reg()`, or one of the relevant [parsnip
models](https://www.tidymodels.org/find/parsnip/):

```{r make-forecasts, warning=FALSE}
two_week_ahead <- arx_forecaster(
  covid_case_death_rates |> filter(time_value <= forecast_date),
  outcome = "death_rate",
  trainer = quantile_reg(),
  predictors = c("death_rate"),
  args_list = arx_args_list(
    lags = list(c(0, 7, 14)),
    ahead = 14
  )
)
hardhat::extract_fit_engine(two_week_ahead$epi_workflow)
```

The default trainer is `parsnip::linear_reg()`, which generates quantiles after
the fact in the post-processing layers, rather than as part of the model.
While this does work, it is generally preferable to use `quantile_reg()`, as the
quantiles generated in post-processing can be poorly behaved.
`quantile_reg()` on the other hand directly estimates a different linear model
for each quantile, reflected in the several different columns for `tau` above.

Because of the flexibility of `{parsnip}`, there are a whole host of models
available to us[^5]; as an example, we could have just as easily substituted a
non-linear random forest model from `{ranger}`:

```{r rand_forest_ex, warning=FALSE}
two_week_ahead <- arx_forecaster(
  covid_case_death_rates |> filter(time_value <= forecast_date),
  outcome = "death_rate",
  trainer = rand_forest(mode = "regression"),
  predictors = c("death_rate"),
  args_list = arx_args_list(
    lags = list(c(0, 7, 14)),
    ahead = 14
  )
)
```

Any other customization is routed through `arx_args_list()`; for example, if we
wanted to increase the number of quantiles fit:

```{r make-quantile-levels-forecasts, warning=FALSE}
two_week_ahead <- arx_forecaster(
  covid_case_death_rates |>
    filter(time_value <= forecast_date, geo_value %in% used_locations),
  outcome = "death_rate",
  trainer = quantile_reg(),
  predictors = c("death_rate"),
  args_list = arx_args_list(
    lags = list(c(0, 7, 14)),
    ahead = 14,
    ############ changing quantile_levels ############
    quantile_levels = c(0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95)
    ##################################################
  )
)
hardhat::extract_fit_engine(two_week_ahead$epi_workflow)
```

See the function documentation for `arx_args_list()` for more examples of the modifications available.
If you want to make further modifications, you will need a custom
workflow; see the ["guts" vignette](custom_epiworkflows) for details.

## Generating multiple aheads
Frequently, one doesn't want just a forecast for a single day, but a trajectory
of forecasts for several weeks.
We can do this with `arx_forecaster()` by looping over aheads; for
example, to predict every day over a 4-week time period:

```{r temp-thing}
all_canned_results <- lapply(
  seq(0, 28),
  \(days_ahead) {
    arx_forecaster(
      covid_case_death_rates |>
        filter(time_value <= forecast_date, geo_value %in% used_locations),
      outcome = "death_rate",
      predictors = c("case_rate", "death_rate"),
      trainer = quantile_reg(),
      args_list = arx_args_list(
        lags = list(c(0, 1, 2, 3, 7, 14), c(0, 7, 14)),
        ahead = days_ahead
      )
    )
  }
)
# pull out the workflow and the predictions to be able to
#  effectively use autoplot
workflow <- all_canned_results[[1]]$epi_workflow
results <- purrr::map_df(all_canned_results, ~ `$`(., "predictions"))
autoplot(
  object = workflow,
  predictions = results,
  plot_data = covid_case_death_rates |>
    filter(geo_value %in% used_locations, time_value > "2021-07-01")
)
```

## Other canned forecasters
### `flatline_forecaster()`
The simplest model we provide is the `flatline_forecaster()`, which predicts a
flat line (with quantiles generated from the residuals using
`layer_residual_quantiles()`).
For example, on the same dataset as above:
```{r make-flatline-forecast, warning=FALSE}
all_flatlines <- lapply(
  seq(0, 28),
  \(days_ahead) {
    flatline_forecaster(
      covid_case_death_rates |>
        filter(time_value <= forecast_date, geo_value %in% used_locations),
      outcome = "death_rate",
      args_list = flatline_args_list(
        ahead = days_ahead,
        quantile_levels = c(0.05, 0.5, 0.95)
      )
    )
  }
)
# same plotting code as in the arx multi-ahead case
workflow <- all_flatlines[[1]]$epi_workflow
results <- purrr::map_df(all_flatlines, ~ `$`(., "predictions"))
results %>% filter(target_date == max(target_date))
autoplot(
  object = workflow,
  predictions = results,
  plot_data = covid_case_death_rates |> filter(geo_value %in% used_locations, time_value > "2021-07-01")
)
```

Note that the `cdc_baseline_forecaster` is a slight modification of this method
for use in [the CDC COVID19 Forecasting Hub](https://covid19forecasthub.org/).

### `arx_classifier()`

The most complicated of the canned forecasters, `arx_classifier` first
translates the outcome into a growth rate, and then classifies that growth rate
into bins.
For example, on the same dataset and `forecast_date` as above, we get:

```{r discrete-rt}
classifier <- arx_classifier(
  covid_case_death_rates |>
    filter(geo_value %in% used_locations, time_value < forecast_date),
  outcome = "death_rate",
  predictors = c("death_rate", "case_rate"),
  trainer = multinom_reg(),
  args_list = arx_class_args_list(
    lags = list(c(0, 1, 2, 3, 7, 14), c(0, 7, 14)),
    ahead = 2 * 7,
    breaks = c(-0.01, 0.01, 0.1)
  )
)
classifier$predictions
```

The prediction splits into 4 cases: `(-∞, -0.01)`, `(-0.01, 0.01)`,  `(0.01,
0.1)`, and `(0.1, ∞)`.
In this case, the classifier put all 4 of the states in the same category,
`(0.01, 0.1)`. **TODO** _effected by the old data._
The number and size of the categories is controlled by `breaks`, which gives the
boundary values.

For comparison, the growth rates for the `target_date`, as computed using
`{epiprocess}`:

```{r growth_rate_results}
growth_rates <- covid_case_death_rates |>
  filter(geo_value %in% used_locations) |>
  group_by(geo_value) |>
  mutate(
    deaths_gr = growth_rate(x = time_value, y = death_rate)
  ) |>
  ungroup()
growth_rates |> filter(time_value == "2021-08-14")
```

Unfortunately, this forecast was not particularly accurate, since for example
`-1.39` is not remotely in the interval `(-0.01, 0.01]`.


## Fitting multi-key panel data

If you have multiple keys that are set in the `epi_df` as `other_keys`,
`arx_forecaster` will automatically group by those as well.
For example, predicting the number of graduates in each of the categories in `grad_employ` from above:

```{r multi_key_forecast, warning=FALSE}
# only fitting a subset, otherwise there are ~550 distinct pairs, which is bad for plotting
edu_quals <- c("Undergraduate degree", "Professional degree")
geo_values <- c("Quebec", "British Columbia")
grad_forecast <- arx_forecaster(
  grad_employ_subset |>
    filter(time_value < 2017) |>
    filter(edu_qual %in% edu_quals, geo_value %in% geo_values),
  outcome = "num_graduates",
  predictors = c("num_graduates"),
  args_list = arx_args_list(
    lags = list(c(0, 1, 2)),
    ahead = 1
  )
)
# and plotting
autoplot(
  grad_forecast$epi_workflow,
  grad_forecast$predictions,
  grad_employ_subset |>
    filter(edu_qual %in% edu_quals, geo_value %in% geo_values),
)
```

The 8 graphs are all pairs of the `geo_values` (`"Quebec"` and `"British Columbia"`), `edu_quals` (`"Undergraduate degree"` and `"Professional degree"`), and age brackets (`"15 to 34 years"` and `"35 to 64 years"`).

## Fitting a non-geo-pooled model

Because our internal methods fit a single model, to fit a non-geo-pooled model
that has a different fit for each geography, one either needs a multi-level
engine (which at the moment parsnip doesn't support), or one needs to map over
geographies.

```{r fit_non_geo_pooled, warning=FALSE}
geo_values <- covid_case_death_rates |>
  pull(geo_value) |>
  unique()

all_fits <-
  purrr::map(geo_values, \(geo) {
    covid_case_death_rates |>
      filter(
        geo_value == geo,
        time_value <= forecast_date
      ) |>
      arx_forecaster(
        outcome = "death_rate",
        trainer = linear_reg(),
        predictors = c("death_rate"),
        args_list = arx_args_list(
          lags = list(c(0, 7, 14)),
          ahead = 14
        )
      )
  })
map_df(all_fits, ~ pluck(., "predictions"))
```

This is both 56 times slower[^7], and uses far less data to fit each model.
If the geographies are at all comparable, for example by normalization, we would
 get much better results by pooling.

If we wanted to build a geo-aware model, such as one that sets the constant in a
 linear regression fit to be different for each geography, we would need to build a [Custom workflow](custom_epiworkflows) with geography as a factor.

# Anatomy of a canned forecaster
## Code object
Let's dissect the forecaster we trained back on the [landing
page](../index.html#motivating-example):

```{r make-four-forecasts, warning=FALSE}
four_week_ahead <- arx_forecaster(
  covid_case_death_rates |> filter(time_value <= forecast_date),
  outcome = "death_rate",
  predictors = c("case_rate", "death_rate"),
  args_list = arx_args_list(
    lags = list(c(0, 1, 2, 3, 7, 14), c(0, 7, 14)),
    ahead = 4 * 7,
    quantile_levels = c(0.1, 0.25, 0.5, 0.75, 0.9)
  )
)
```

`four_week_ahead` has two components: an `epi_workflow`, and a table of
`predictions`.
The table of predictions is simply a tibble of the predictions,

```{r show_predictions}
four_week_ahead$predictions
```

`.pred` gives the point/median prediction, while `.pred_distn` is a
`dist_quantiles()` object representing a distribution through various quantile
levels.
The `[6]` in the name refers to the number of quantiles that have been
explicitly created[^4]; by default, this covers a 90% prediction interval, or 5%
and 95%.

The `epi_workflow` is a significantly more complicated object, extending a
`workflows::workflow()`  to include post-processing:

```{r show_workflow}
four_week_ahead$epi_workflow
```

An `epi_workflow()` consists of 3 parts:

- `Preprocessor`: a collection of steps that transform the data to be ready for
      modelling. They come from this package or [any of the recipes
      steps](https://recipes.tidymodels.org/reference/index.html);
      `four_week_ahead` has 5 of these, and you can inspect them more closely by
      running `hardhat::extract_recipe(four_week_ahead$epi_workflow)`.[^6]
- `Model`: the actual model that does the fitting, given by a
  `parsnip::model_spec`; `four_week_ahead` has the default of
  `parsnip::linear_reg()`, which is a wrapper from `{parsnip}` for
  `stats::lm()`. You can inspect the model more closely by running
  `hardhat::extract_fit_recipe(four_week_ahead$epi_workflow)`.
- `Postprocessor`: a collection of layers to be applied to the resulting
  forecast, internal to this package. `four_week_ahead` just so happens to have
  5 of as these well. You can inspect the layers more closely by running
  `epipredict::extract_layers(four_week_ahead$epi_workflow)`.

See the ["guts" vignette](custom_epiworkflows) for recreating and then
extending `four_week_ahead` using the custom forecaster framework.

## Mathematical description

Let's describe in more detail the actual fit model for a more minimal version of
`four_week_ahead`:

```{r, four_week_again}
four_week_small <- arx_forecaster(
  covid_case_death_rates |> filter(time_value <= forecast_date),
  outcome = "death_rate",
  predictors = c("case_rate", "death_rate"),
  args_list = arx_args_list(
    lags = list(c(0, 7, 14), c(0, 7, 14)),
    ahead = 4 * 7,
    quantile_levels = c(0.1, 0.25, 0.5, 0.75, 0.9)
  )
)
hardhat::extract_fit_engine(four_week_small$epi_workflow)
```

If $d_t$ is the death rate on day $t$ and $c_t$ is the case rate, then the model
we're fitting is:

$$
d_{t+28} = a_0 + a_1 d_t + a_2 d_{t-7} + a_3 d_{t-14} + a_4 c_t + a_5 c_{t-7} + a_6 c_{t-14}.
$$

For example, $a_1$ is `lag_0_death_rate` above, with a value of `r hardhat::extract_fit_engine(four_week_small$epi_workflow)$coefficients["lag_0_death_rate"] `,
while $a_5$ is `r hardhat::extract_fit_engine(four_week_small$epi_workflow)$coefficients["lag_7_case_rate"] `.

The training data for fitting this linear model is created by creating a series
of columns shifted by the appropriate amount; this makes it so that each row
without `NA` values is a training point to fit the coefficients $a_0,\ldots, a_6$.

[^4]: in the case of a `{parsnip}` engine which doesn't explicitly predict
    quantiles, these quantiles are created using `layer_residual_quantiles()`,
    which infers the quantiles from the residuals of the fit.

[^5]: in the case of `arx_forecaster`, this is any model with
    `mode="regression"` from [this
    list](https://www.tidymodels.org/find/parsnip/).

[^6]: alternatively, for an unfit version of the preprocessor, you can call
    `hardhat::extract_preprocessor(four_week_ahead$epi_workflow)`

[^7]: the number of geographies
