---
title: "Guts"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Guts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
source("_common.R")
```

```{r setup, message=FALSE, include = FALSE}
library(dplyr)
library(parsnip)
library(workflows)
library(recipes)
library(epipredict)
forecast_date <- as.Date("2021-08-01")
used_locations <- c("ca", "ma", "ny", "tx")
library(epidatr)
cases <- pub_covidcast(
  source = "jhu-csse",
  signals = "confirmed_incidence_prop",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20200601, 20211231),
  geo_values = "*"
) |>
  select(geo_value, time_value, case_rate = value)

deaths <- pub_covidcast(
  source = "jhu-csse",
  signals = "deaths_incidence_prop",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20200601, 20211231),
  geo_values = "*"
) |>
  select(geo_value, time_value, death_rate = value)
cases_deaths <-
  full_join(cases, deaths, by = c("time_value", "geo_value")) |>
  filter(geo_value %in% used_locations) |>
  as_epi_df(as_of = as.Date("2022-01-01"))
cases_deaths <-
  cases_deaths |>
  group_by(geo_value) |>
  epi_slide(
    cases_7dav = mean(case_rate, na.rm = TRUE),
    death_rate_7dav = mean(death_rate, na.rm = TRUE),
    .window_size = 7
  ) |>
  ungroup() |>
  mutate(case_rate = NULL, death_rate = NULL) |>
  rename(case_rate = cases_7dav, death_rate = death_rate_7dav)
cases_deaths <-
  cases_deaths |>
  group_by(geo_value) |>
  mutate(
    outlr_death_rate = detect_outlr_rm(
      time_value, death_rate,
      detect_negatives = TRUE
    ),
    outlr_case_rate = detect_outlr_rm(
      time_value, case_rate,
      detect_negatives = TRUE
    )
  ) |>
  unnest(cols = starts_with("outlr"), names_sep = "_") |>
  ungroup() |>
  mutate(
    death_rate = outlr_death_rate_replacement,
    case_rate = outlr_case_rate_replacement
  ) |>
  select(geo_value, time_value, case_rate, death_rate)
jhu <- covid_case_death_rates
out_gb <- arx_forecaster(
  jhu,
  outcome = "death_rate",
  predictors = c("case_rate", "death_rate"),
  trainer = boost_tree(mode = "regression", trees = 20)
)
out_q <- arx_forecaster(jhu, "death_rate", c("case_rate", "death_rate"),
  args_list = arx_args_list(
    quantile_levels = c(.01, .025, seq(.05, .95, by = .05), .975, .99)
  )
)
```
# Document Title
## Inner workings

Underneath the hood, this forecaster creates (and returns) an `epi_workflow`.
Essentially, this is a big S3 object that wraps up the 4 modular steps
(preprocessing - postprocessing) described above.

### Preprocessing

Preprocessing is accomplished through a `recipe` (imagine baking a cake) as
provided in the [`{recipes}`](https://recipes.tidymodels.org) package.
We've made a few modifications (to handle
panel data) as well as added some additional options. The recipe gives a
specification of how to handle training data. Think of it like a fancified
`formula` that you would pass to `lm()`: `y ~ x1 + log(x2)`. In general,
there are 2 extensions to the `formula` that `{recipes}` handles:

  1. Doing transformations of both training and test data that can always be
  applied. These are things like taking the log of a variable, leading or
  lagging, filtering out rows, handling dummy variables, etc.
  2. Using statistics from the training data to eventually process test data.
    This is a major benefit of `{recipes}`. It prevents what the tidy team calls
    "data leakage". A simple example is centering a predictor by its mean. We
    need to store the mean of the predictor from the training data and use that
    value on the test data rather than accidentally calculating the mean of
    the test predictor for centering.

A recipe is processed in 2 steps, first it is "prepped". This calculates and
stores any intermediate statistics necessary for use on the test data.
Then it is "baked"
resulting in training data ready for passing into a statistical model (like `lm`).

We have introduced an `epi_recipe`. It's just a `recipe` that knows how to handle
the `time_value`, `geo_value`, and any additional keys so that these are available
when necessary.

The `epi_recipe` from `out_gb` can be extracted from the result:

```{r}
extract_recipe(out_gb$epi_workflow)
```

The "Inputs" are the original `epi_df` and the "roles" that these are assigned.
None of these are predictors or outcomes. Those will be created
by the recipe when it is prepped. The "Operations" are the sequence of
instructions to create the cake (baked training data).
Here we create lagged predictors, lead the outcome, and then remove `NA`s.
Some models like `lm` internally handle `NA`s, but not everything does, so we
deal with them explicitly. The code to do this (inside the forecaster) is

```{r}
er <- epi_recipe(jhu) %>%
  step_epi_lag(case_rate, death_rate, lag = c(0, 7, 14)) %>%
  step_epi_ahead(death_rate, ahead = 7) %>%
  step_epi_naomit()
```

While `{recipes}` provides a function `step_lag()`, it assumes that the data
have no breaks in the sequence of `time_values`. This is a bit dangerous, so
we avoid that behaviour. Our `lag/ahead` functions also appropriately adjust the
amount of data to avoid accidentally dropping recent predictors from the test
data.

### The model specification

Users with familiarity with the `{parsnip}` package will have no trouble here.
Basically, `{parsnip}` unifies the function signature across statistical models.
For example, `lm()` "likes" to work with formulas, but `glmnet::glmnet()` uses
`x` and `y` for predictors and response. `{parsnip}` is agnostic. Both of these
do "linear regression". Above we switched from `lm()` to `xgboost()` without
any issue despite the fact that these functions couldn't be more different.

```{r, eval = FALSE}
lm(formula, data, subset, weights, na.action,
  method = "qr",
  model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,
  contrasts = NULL, offset, ...
)

xgboost(
  data = NULL, label = NULL, missing = NA, weight = NULL,
  params = list(), nrounds, verbose = 1, print_every_n = 1L,
  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,
  save_name = "xgboost.model", xgb_model = NULL, callbacks = list(),
  ...
)
```

`{epipredict}` provides a few engines/modules (the flatline forecaster and
quantile regression), but you should be able to use any available models
listed [here](https://www.tidymodels.org/find/parsnip/).

To estimate (fit) a preprocessed model, one calls `fit()` on the `epi_workflow`.

```{r}
ewf <- epi_workflow(er, linear_reg()) %>% fit(jhu)
```

### Postprocessing

To stretch the metaphor of preparing a cake to its natural limits, we have
created postprocessing functionality called "frosting". Much like the recipe,
each postprocessing operation is a "layer" and we "slather" these onto our
baked cake. To fix ideas, below is the postprocessing `frosting` for
`arx_forecaster()`

```{r}
extract_frosting(out_q$epi_workflow)
```

Here we have 5 layers of frosting. The first generates the forecasts from the test data.
The second uses quantiles of the residuals to create distributional
forecasts. The next two add columns for the date the forecast was made and the
date for which it is intended to occur. Because we are predicting rates, they
should be non-negative, so the last layer thresholds both predicted values and
intervals at 0. The code to do this (inside the forecaster) is

```{r}
f <- frosting() %>%
  layer_predict() %>%
  layer_residual_quantiles(
    quantile_levels = c(.01, .025, seq(.05, .95, by = .05), .975, .99),
    symmetrize = TRUE
  ) %>%
  layer_add_forecast_date() %>%
  layer_add_target_date() %>%
  layer_threshold(starts_with(".pred"))
```

At predict time, we add this object onto the `epi_workflow` and call `forecast()`

```{r, warning=FALSE}
ewf %>%
  add_frosting(f) %>%
  forecast()
```

The above `get_test_data()` function examines the recipe and ensures that enough
test data is available to create the necessary lags and produce a prediction
for the desired future time point (after the end of the training data). This mimics
what would happen if `jhu` contained the most recent available historical data and
we wanted to actually predict the future. We could have instead used any test data
that contained the necessary predictors.


## Conclusion

Internally, we provide some simple functions to create reasonable forecasts.
But ideally, a user could create their own forecasters by building up the
components we provide. In other vignettes, we try to walk through some of these
customizations.

To illustrate everything above, here is (roughly) the code for the
`flatline_forecaster()` applied to the `case_rate`.

```{r}
r <- epi_recipe(jhu) %>%
  step_epi_ahead(case_rate, ahead = 7, skip = TRUE) %>%
  update_role(case_rate, new_role = "predictor") %>%
  add_role(all_of(key_colnames(jhu)), new_role = "predictor")

f <- frosting() %>%
  layer_predict() %>%
  layer_residual_quantiles() %>%
  layer_add_forecast_date() %>%
  layer_add_target_date() %>%
  layer_threshold(starts_with(".pred"))

eng <- linear_reg() %>% set_engine("flatline")
wf <- epi_workflow(r, eng, f) %>% fit(jhu)
preds <- forecast(wf)
```

All that really differs from the `arx_forecaster()` is the `recipe`, the
test data, and the engine. The `frosting` is identical, as is the fitting
and predicting procedure.

```{r}
preds
```

